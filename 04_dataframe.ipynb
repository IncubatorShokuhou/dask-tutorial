{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"http://dask.readthedocs.io/en/latest/_images/dask_horizontal.svg\"\n",
    "     align=\"right\"\n",
    "     width=\"30%\"\n",
    "     alt=\"Dask logo\\\">\n",
    "\n",
    "\n",
    "# Dask DataFrames\n",
    "\n",
    "我们通过使用 dask.delayed 在 CSV 文件目录上构建并行数据帧计算完成了第 1 章。 在本节中，我们使用 `dask.dataframe` 自动构建类似的计算，用于表格计算的常见情况。 Dask DataFrame的外观和感觉与 Pandas 数据帧相似，但Dask DataFrame运行在支持`dask.delayed`的相同基础架构上。\n",
    "\n",
    "在这个笔记本中，我们像以前一样使用相同的航线数据，但是现在我们让`dask.dataframe’`为我们构造计算，而不是写 for循环。函数可以接受`data/nycflights/*`这样的全局字符串,然后在我们所有的数据上建立并行计算。\n",
    "\n",
    "## 何时使用 `dask.dataframe`\n",
    "\n",
    "Pandas非常适合存储在内存中的表格数据集。当要分析的数据集大于机器的内存时，Dask 就变得有用了。我们使用的演示数据集大约只有200MB，因此你可以在合理的时间内下载它，但是`dask.dataframe`将扩展到比内存大得多的数据集。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"images/pandas_logo.png\" align=\"right\" width=\"28%\">\n",
    "\n",
    "`dask.dataframe` 模块实现了一个分块的并行 `DataFrame` 对象，它模仿了 Pandas `DataFrame` API 的一个大集合。 一个 Dask `DataFrame` 由许多沿索引分隔的内存中的 Pandas `DataFrames` 组成。 Dask `DataFrame` 上的一个操作会以一种注意潜在并行性和内存限制的方式触发对组成 Pandas `DataFrame` 的许多 Pandas 操作。\n",
    "\n",
    "**相关文档**\n",
    "\n",
    "* [DataFrame 文档](https://docs.dask.org/en/latest/dataframe.html)\n",
    "* [DataFrame 屏幕录像](https://youtu.be/AT2XtFehFSQ)\n",
    "* [DataFrame API](https://docs.dask.org/en/latest/dataframe-api.html)\n",
    "* [DataFrame 示例](https://examples.dask.org/dataframe.html)\n",
    "* [Pandas 文档](https://pandas.pydata.org/pandas-docs/stable/)\n",
    "\n",
    "**主要知识**\n",
    "\n",
    "1.  Dask DataFrame对pandas用户来说应该很熟悉\n",
    "2.  数据流的划分对于有效执行非常重要"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 创建数据"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%run prep.py -d flights"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 设置"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们创建了人造数据。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from prep import accounts_csvs\n",
    "accounts_csvs()\n",
    "\n",
    "import os\n",
    "import dask\n",
    "filename = os.path.join('data', 'accounts.*.csv')\n",
    "filename"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "文件名包含一个通配符`*`，因此路径中匹配该通配符的所有文件将被读入同一个的 Dask DataFrame。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import dask.dataframe as dd\n",
    "df = dd.read_csv(filename)\n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 加载并计算行数\n",
    "len(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "这里发生了什么？\n",
    "- Dask调查了输入路径，发现有3个匹配的文件\n",
    "- 为每个块智能地创建了一组任务 - 在这种情况下每个原始 CSV 文件对应一个任务\n",
    "- 每个文件都被加载到一个 Pandas 数据帧中，并应用了 `len()`\n",
    "- 小计被合并为您最终的总计。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 真实数据\n",
    "\n",
    "让我们以几年来在美国的航班为例来尝试一下。这些数据是专门针对纽约市地区三个机场的航班。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = dd.read_csv(os.path.join('data', 'nycflights', '*.csv'),\n",
    "                 parse_dates={'Date': [0, 1, 2]})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "请注意，DataFrame对象的描述不包含任何数据 —— Dask 刚刚读取了第一个文件的开头，并推断出了列名和 dtype。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们可以查看数据的首尾"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.tail()  # 该操作会失败"
   ],
   "outputs": [],
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 刚才发生了什么？\n",
    "\n",
    "与在推断数据类型之前读取整个文件的 `pandas.read_csv` 不同，`dask.dataframe.read_csv` 仅从文件的开头（或第一个文件，如果使用通配）读取样本。然后在读取所有分区时强制执行这些推断的数据类型。\n",
    "\n",
    "在这种情况下，样本中推断的数据类型不正确。前n行中`CRSElapsedTime`没有值（pandas 将其推断为`float`），后来变成字符串（`object` dtype）。请注意，Dask 会提供有关不匹配的信息性错误消息。发生这种情况时，您有几个选择：\n",
    "\n",
    "- 直接使用 `dtype` 关键字指定 dtypes。这是推荐的解决方案，因为它最不容易出错（显式比隐式更好），而且性能最高。\n",
    "- 增加 `sample` 关键字的大小（以字节为单位）\n",
    "- 使用 `assume_missing` 使 `dask` 假定推断为 `int`（不允许缺失值）的列实际上是浮点数（允许缺失值）。在我们的特殊情况下，这不适用。\n",
    "\n",
    "在我们的例子中，我们将使用第一个选项并直接指定报错列的 `dtypes`。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = dd.read_csv(os.path.join('data', 'nycflights', '*.csv'),\n",
    "                 parse_dates={'Date': [0, 1, 2]},\n",
    "                 dtype={'TailNum': str,\n",
    "                        'CRSElapsedTime': float,\n",
    "                        'Cancelled': bool})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.tail()  # 现在运行成功了"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 使用 `dask.dataframe` 计算\n",
    "\n",
    "我们计算`DepDelay`列的最大值。 仅使用pandas的情况下，我们将遍历每个文件以找到各个最大值，然后在所有各个最大值上找到最终最大值\n",
    "\n",
    "```python\n",
    "maxes = []\n",
    "for fn in filenames:\n",
    "    df = pd.read_csv(fn)\n",
    "    maxes.append(df.DepDelay.max())\n",
    "    \n",
    "final_max = max(maxes)\n",
    "```\n",
    "\n",
    "我们可以用`dask.delayed`来包装`pd.read_csv`使其并行运行。 无论如何，我们仍然必须考虑循环，中间结果（每个文件一个）和最终减少量（中间最大值的`max`）。这只是围绕真实任务的噪音，pandas用以下代码来解决\n",
    "\n",
    "```python\n",
    "df = pd.read_csv(filename, dtype=dtype)\n",
    "df.DepDelay.max()\n",
    "```\n",
    "\n",
    "`dask.dataframe` 让我们编写类似于pandas的代码，该代码可以并行处理大于内存数据集的操作。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%time df[\"DepDelay\"].max().compute()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "这会为我们编写延迟计算，然后运行它。\n",
    "\n",
    "一些注意事项：\n",
    "\n",
    "1.与`dask.delayed`一样，我们需要在完成后调用`.compute()`。 到目前为止，一切都是惰性的。\n",
    "2. Dask 将尽快删除中间结果（如每个文件的完整 Pandas 数据框）。\n",
    "     - 这让我们可以处理大于内存的数据集\n",
    "     - 这意味着每次重复计算都必须加载所有数据（再次运行上面的代码，它是否比您预期的更快或更慢？）\n",
    "    \n",
    "与`delayed`对象一样，您可以使用`.visualize`方法查看底层任务图："
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 注意并行\n",
    "df[\"DepDelay\"].max().visualize()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 练习\n",
    "\n",
    "本节中我们进行了一些 `dask.dataframe` 计算. 如果您对pandas很适应，那么这些应该很熟悉. 您将需要思考何时调用 `compute`.\n",
    "\n",
    "### 1.) 数据集有多少行?\n",
    "\n",
    "如果您不熟悉pandas，您将如何检查元组列表中有多少条记录？"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 在这儿输入你的代码"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(df)"
   ],
   "outputs": [],
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.) 总共有多少非取消（non-canceled）航班？\n",
    "\n",
    "用pandas的话，您需要使用 [boolean indexing](https://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 在这儿输入你的代码"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(df[~df.Cancelled])"
   ],
   "outputs": [],
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.) 每个机场总共有多少非取消（non-canceled）航班？\n",
    "\n",
    "*提示*: 使用 [`df.groupby`](https://pandas.pydata.org/pandas-docs/stable/groupby.html)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 在这儿输入你的代码"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df[~df.Cancelled].groupby('Origin').Origin.count().compute()"
   ],
   "outputs": [],
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.) 每个机场的平均起飞延误是多少？\n",
    "\n",
    "请注意，这与您在之前的笔记本中所做的计算相同（这种方法是更快还是更慢？）"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 在这儿输入你的代码"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.groupby(\"Origin\").DepDelay.mean().compute()"
   ],
   "outputs": [],
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.) 一周中哪一天的平均出发延误最严重？"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 在这儿输入你的代码"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.groupby(\"DayOfWeek\").DepDelay.mean().compute()"
   ],
   "outputs": [],
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 共享中间结果\n",
    "\n",
    "在计算上述所有内容时，我们有时会多次执行相同的操作。 对于大多数操作，`dask.dataframe` 散列参数，允许共享重复计算，并且只计算一次。\n",
    "\n",
    "例如，让我们计算所有未取消航班的出发延迟的平均值和标准偏差。 由于 dask 操作是惰性的，因此这些值还不是最终结果。 它们只是获得结果所需的配方。\n",
    "\n",
    "如果我们通过两次计算调用来计算它们，则中间计算不会共享。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "non_cancelled = df[~df.Cancelled]\n",
    "mean_delay = non_cancelled.DepDelay.mean()\n",
    "std_delay = non_cancelled.DepDelay.std()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "\n",
    "mean_delay_res = mean_delay.compute()\n",
    "std_delay_res = std_delay.compute()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "但是让我们尝试将两者都传递给单个 `compute` 调用。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "\n",
    "mean_delay_res, std_delay_res = dask.compute(mean_delay, std_delay)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "使用 `dask.compute` 大约需要 1/2 的时间。 这是因为在调用 `dask.compute` 时合并了两个结果的任务图，允许共享操作只执行一次而不是两次。 特别是，使用 `dask.compute` 只执行以下一次：\n",
    "\n",
    "- 调用 `read_csv`\n",
    "- 过滤器（`df[~df.Cancelled]`）\n",
    "- 一些必要的归约（`sum`，`count`）\n",
    "\n",
    "要查看多个结果之间的合并任务图是什么样的（以及共享的内容），您可以使用 `dask.visualize` 函数（我们可能希望使用 `filename='graph.pdf'` 将图形保存到磁盘，以便我们可以更轻松地放大）："
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dask.visualize(mean_delay, std_delay)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 这与 Pandas 相比如何？"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pandas 比 dask.dataframe 更成熟、功能更齐全。如果您的数据适合内存，那么您应该使用 Pandas。当您对不适合内存的数据集进行操作时，`dask.dataframe` 模块为您提供了有限的 `pandas` 体验。\n",
    "\n",
    "在本教程中，我们提供了一个由几个 CSV 文件组成的小数据集。这个数据集在磁盘上有 45MB，在内存中扩展到大约 400MB。该数据集足够小，您通常可以使用 Pandas。\n",
    "\n",
    "我们选择了这个尺寸，以便练习快速完成。 Dask.dataframe 只有对比这大得多的问题才真正变得有意义，此时，Pandas 打破了可怕的\n",
    "\n",
    "    MemoryError:  ...\n",
    "    \n",
    "此外，分布式调度器允许跨集群执行相同的数据帧表达式。为了实现海量“大数据”处理，可以执行数据摄取功能，例如`read_csv`，其中数据保存在每个工作节点（例如亚马逊的 S3）都可以访问的存储中，并且因为大多数操作从仅选择一些列开始，转换和过滤数据，只需要在机器之间通信相对少量的数据。\n",
    "\n",
    "Dask.dataframe 操作在内部使用 `pandas` 操作。除了以下两种情况外，它们通常以大致相同的速度运行：\n",
    "\n",
    "1. Dask 引入了一些开销，每个任务大约 1 毫秒。这通常可以忽略不计。\n",
    "2. 当 Pandas释放GIL 时，`dask.dataframe` 可以在一个进程内并行调用多个 Pandas 操作，提高速度与内核数量成正比。对于不释放 GIL 的操作，需要多个进程才能获得相同的加速。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dask DataFrame 数据模型\n",
    "\n",
    "在大多数情况下，Dask DataFrame 感觉就像一个 Pandas DataFrame。\n",
    "到目前为止，我们看到的最大区别是 Dask 操作是惰性的； 他们建立了一个任务图而不是立即执行（更多细节见 [调度器](05_distributed.ipynb)）。\n",
    "这让 Dask 可以在内核外并行执行操作。\n",
    "\n",
    "在[Dask Arrays](03_array.ipynb)中, 我们看到一个 `dask.array` 由许多 NumPy 数组组成，沿着一个或多个维度分块。\n",
    "在`dask.dataframe`中也是相似的: Dask DataFrame 由许多 Pandas DataFrame 组成。 对于`dask.dataframe`，分块仅沿索引发生。\n",
    "\n",
    "<img src=\"http://dask.pydata.org/en/latest/_images/dask-dataframe.svg\" width=\"30%\">\n",
    "\n",
    "我们称每个块为 *partition*，上限/下限为 *division*。\n",
    "Dask *可以* 存储有关division的信息。 目前，当您编写自定义函数以应用于 Dask DataFrame 时会出现分区。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 将 `CRSDepTime` 转化为时间戳\n",
    "\n",
    "该数据集将时间戳存储为`HHMM`，在`read_csv`中作为整数读入："
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "crs_dep_time = df.CRSDepTime.head(10)\n",
    "crs_dep_time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "要将这些转换为预定出发时间的时间戳，我们需要将这些整数转换为 `pd.Timedelta` 对象，然后将它们与 `Date` 列组合。\n",
    "\n",
    "在 Pandas 中，我们会使用 `pd.to_timedelta` 函数和一些算术来做到这一点："
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 获取前 10 个日期来补充我们的 `crs_dep_time`\n",
    "date = df[\"Date\"].head(10)\n",
    "\n",
    "# 以整数形式获取小时数，转换为时间增量(timedelta)\n",
    "hours = crs_dep_time // 100\n",
    "hours_timedelta = pd.to_timedelta(hours, unit='h')\n",
    "\n",
    "# 以整数形式获取分钟数，转换为时间增量(timedelta)\n",
    "minutes = crs_dep_time % 100\n",
    "minutes_timedelta = pd.to_timedelta(minutes, unit='m')\n",
    "\n",
    "# 应用 timedeltas 以按出发时间偏移日期\n",
    "departure_timestamp = date + hours_timedelta + minutes_timedelta\n",
    "departure_timestamp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 自定义代码和Dask Dataframe\n",
    "\n",
    "我们可以将 `pd.to_timedelta` 换成 `dd.to_timedelta`，并对整个 dask DataFrame 执行相同的操作。 但是假设 Dask 还没有实现适用于 Dask DataFrame 的 `dd.to_timedelta`。 那你会怎么做？\n",
    "\n",
    "`dask.dataframe` 提供了一些方法来更容易地将自定义函数应用于 Dask DataFrames：\n",
    "\n",
    "- [`map_partitions`](http://dask.pydata.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.map_partitions)\n",
    "- [`map_overlap`](http://dask.pydata.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.map_overlap)\n",
    "- [`reduction`](http://dask.pydata.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.reduction)\n",
    "\n",
    "在这里，我们将只讨论 `map_partitions`，我们可以使用它来自己实现 `to_timedelta`："
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 查看`map_partitions`的文档\n",
    "\n",
    "help(df[\"CRSDepTime\"].map_partitions)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "基本思想是将一个对 DataFrame 进行操作的函数应用于每个分区。\n",
    "在这种情况下，我们将应用 `pd.to_timedelta`。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "hours = df[\"CRSDepTime\"] // 100\n",
    "# hours_timedelta = pd.to_timedelta(hours, unit='h')\n",
    "hours_timedelta = hours.map_partitions(pd.to_timedelta, unit='h')\n",
    "\n",
    "minutes = df[\"CRSDepTime\"] % 100\n",
    "# minutes_timedelta = pd.to_timedelta(minutes, unit='m')\n",
    "minutes_timedelta = minutes.map_partitions(pd.to_timedelta, unit='m')\n",
    "\n",
    "departure_timestamp = df[\"Date\"] + hours_timedelta + minutes_timedelta"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "departure_timestamp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "departure_timestamp.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 练习：重写上面的代码以使用对 `map_partitions` 的单个调用\n",
    "\n",
    "这将比两个单独的调用稍微更有效，因为它减少了图中的任务数量。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def compute_departure_timestamp(df):\n",
    "    pass  # 目标：完成它"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "departure_timestamp = df.map_partitions(compute_departure_timestamp)\n",
    "\n",
    "departure_timestamp.head()"
   ],
   "outputs": [],
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def compute_departure_timestamp(df):\n",
    "    hours = df.CRSDepTime // 100\n",
    "    hours_timedelta = pd.to_timedelta(hours, unit='h')\n",
    "\n",
    "    minutes = df.CRSDepTime % 100\n",
    "    minutes_timedelta = pd.to_timedelta(minutes, unit='m')\n",
    "\n",
    "    return df.Date + hours_timedelta + minutes_timedelta\n",
    "\n",
    "departure_timestamp = df.map_partitions(compute_departure_timestamp)\n",
    "departure_timestamp.head()"
   ],
   "outputs": [],
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 局限性"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 什么不起作用？"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dask.dataframe 只涵盖了 Pandas API 中较小，但广泛使用的部分。\n",
    "这种限制有两个原因：\n",
    "\n",
    "1. Pandas API真的*很多*\n",
    "2. 一些操作真的很难并行执行（例如排序）\n",
    "\n",
    "此外，一些重要的操作，如 ``set_index`` 可以用dask.dataframe实现，但与 Pandas 相比，速度会较慢，因为它们包括大量的数据混洗，并且可能会写出到磁盘。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 了解更多\n",
    "\n",
    "\n",
    "* [DataFrame 文档](https://docs.dask.org/en/latest/dataframe.html)\n",
    "* [DataFrame 屏幕录像](https://youtu.be/AT2XtFehFSQ)\n",
    "* [DataFrame API](https://docs.dask.org/en/latest/dataframe-api.html)\n",
    "* [DataFrame 示例](https://examples.dask.org/dataframe.html)\n",
    "* [Pandas 文档](https://pandas.pydata.org/pandas-docs/stable/)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "client.shutdown()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('dask-tutorial': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "3f2e3fb42896b6b47c37375014a159d976fd49965917325c599b06eb4946867a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}